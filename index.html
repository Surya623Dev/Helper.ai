<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Interview Co-Pilot — Live</title>
  <style>
    :root{
      --bg:#0f1724;
      --card:#0b1220;
      --muted:#9aa6bf;
      --accent:#0ea5a4;
      --white:#e6eef6;
      font-family: Inter, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
    }
    body{ margin:0; min-height:100vh; background:linear-gradient(180deg,#071124 0%, #0d1724 100%); color:var(--white); display:flex; align-items:center; justify-content:center; padding:32px; }
    .card{ width:900px; background:rgba(255,255,255,0.03); border-radius:12px; padding:20px; box-shadow:0 8px 30px rgba(2,6,23,0.6); }
    header{ display:flex; align-items:center; justify-content:space-between; margin-bottom:12px; }
    h1{ font-size:20px; margin:0; }
    .controls{ display:flex; gap:12px; align-items:center; }
    button{ background:var(--accent); border:none; color:#042024; padding:8px 14px; border-radius:8px; cursor:pointer; font-weight:600; }
    .status{ color:var(--muted); font-size:13px; display:block; margin-top:6px; }
    .mic-icon{ display:inline-block; width:18px; height:18px; border-radius:4px; background:linear-gradient(90deg,#38bdf8,#06b6d4); margin-right:8px; vertical-align:middle; }
    .grid{ display:grid; grid-template-columns: 1fr 360px; gap:18px; margin-top:18px; }
    .panel{ background:rgba(255,255,255,0.02); padding:14px; border-radius:10px; min-height:180px; }
    #transcription-output{ white-space:pre-wrap; font-size:15px; color:var(--white); min-height:120px; }
    #suggestions-container.hidden{ display:none; }
    ul#suggestions-list{ margin:0; padding-left:18px; color:var(--muted); }
    #loading-indicator{ font-size:13px; color:var(--muted); display:inline-block; margin-top:8px; }
    .small{ font-size:12px; color:var(--muted); }
    #initial-message{ color:var(--muted); margin-top:10px; font-size:13px; }
  </style>
</head>
<body>
  <div class="card" role="main">
    <header>
      <div>
        <h1>Interview Co-Pilot — Live</h1>
        <span id="status-message" class="status">Ready. Click Start.</span>
      </div>
      <div class="controls">
        <div class="small">
          <span id="mic-icon" class="mic-icon" title="mic status"></span>
        </div>
        <button id="toggle-mic-btn">Start</button>
      </div>
    </header>

    <div class="grid">
      <div class="panel">
        <strong>Live Transcription</strong>
        <div id="transcription-output" aria-live="polite"></div>
        <div id="initial-message">Click <em>Start</em> and grant microphone access. Transcription will appear here.</div>
      </div>

      <aside class="panel" id="suggestions-container" aria-live="polite">
        <strong>Co-Pilot Suggestions</strong>
        <div id="loading-indicator" class="small hidden">Generating...</div>
        <ul id="suggestions-list"></ul>
      </aside>
    </div>

    <footer style="margin-top:14px; display:flex; justify-content:space-between; align-items:center;">
      <div class="small">Audio encoding: LINEAR16 • Target sample rate: 16 kHz</div>
      <div class="small">AudioWorklet is now used for stable processing.</div>
    </footer>
  </div>

  <script type="module">
    // === CONFIG ===
    // Your Netlify function (or any server endpoint) that returns the key and config data:
    const NETLIFY_FUNCTION_URL = "https://cohelper.netlify.app/.netlify/functions/get-gemini-token";

    const TARGET_SAMPLE_RATE = 16000;
    // NOTE: SYSTEM_INSTRUCTION_PROMPT is now retrieved from the backend function.

    // === DOM ===
    const statusMessage = document.getElementById('status-message');
    const toggleMicBtn = document.getElementById('toggle-mic-btn');
    const micIcon = document.getElementById('mic-icon');
    const transcriptionOutput = document.getElementById('transcription-output');
    const suggestionsContainer = document.getElementById('suggestions-container');
    const suggestionsList = document.getElementById('suggestions-list');
    const loadingIndicator = document.getElementById('loading-indicator');
    const initialMessage = document.getElementById('initial-message');

    // === STATE ===
    let isStreaming = false;
    let webSocket = null;
    let audioContext = null;
    let workletNode = null; 
    let mediaStream = null;
    let currentTranscription = '';
    let isGenerating = false;

    // === Helpers ===

    /**
     * Convert an ArrayBuffer to base64 safely by chunking to avoid apply() limits.
     * Returns a base64 string.
     */
    function bufferToBase64(buffer) {
      const bytes = new Uint8Array(buffer);
      const chunkSize = 0x8000; // 32KB
      let binary = '';
      for (let i = 0; i < bytes.length; i += chunkSize) {
        const chunk = bytes.subarray(i, i + chunkSize);
        binary += String.fromCharCode.apply(null, chunk);
      }
      return btoa(binary);
    }

    // === Networking ===

    /**
     * Fetches API Key, WebSocket URL, Model ID, and System Instruction from the backend.
     */
    async function fetchEphemeralToken() {
      statusMessage.textContent = 'Fetching API key...';
      try {
        const response = await fetch(NETLIFY_FUNCTION_URL);
        if (!response.ok) {
          const errorBody = await response.text().catch(() => `HTTP ${response.status}`);
          throw new Error(`Token fetch failed: ${errorBody}`);
        }
        const data = await response.json();
        // EXPECTING: { apiKey, websocketUrl, targetLiveModel, systemInstruction }
        if (!data || !data.apiKey || !data.websocketUrl || !data.systemInstruction || !data.targetLiveModel) {
            throw new Error('Invalid token response from server: Missing required fields (apiKey, websocketUrl, systemInstruction, or targetLiveModel).');
        }
        return data; 
      } catch (err) {
        console.error('fetchEphemeralToken error', err);
        statusMessage.textContent = 'Failed to fetch API key. Check server.';
        throw err;
      }
    }

    /**
     * Sets up the WebSocket connection using the dynamic data fetched from the backend.
     */
    function setupWebSocket(data) {
      // Use the API Key from the response to build the URL
      const wsUrl = `${data.websocketUrl}?api_key=${encodeURIComponent(data.apiKey)}`;
      webSocket = new WebSocket(wsUrl);

      webSocket.onopen = () => {
        statusMessage.textContent = 'Connected. Sending configuration...';
        initialMessage.classList.add('hidden');

        // First setup/config message for the realtime session, using dynamic data
        const setupPayload = {
          audio_config: {
            sample_rate: TARGET_SAMPLE_RATE,
            encoding: "LINEAR16",
            language_code: "en-US",
          },
          llm_config: {
            model_id: data.targetLiveModel,
            system_instruction: data.systemInstruction,
          }
        };

        const firstMessage = {
          bidiGenerateContentRealtimeInput: {
            liveConfig: setupPayload,
          }
        };

        webSocket.send(JSON.stringify(firstMessage));
        statusMessage.textContent = 'Listening for Interview Audio...';
        micIcon.style.boxShadow = '0 0 8px rgba(6,182,212,0.8)';
        isStreaming = true;
      };

      webSocket.onmessage = handleWebSocketMessage;

      webSocket.onclose = () => {
        statusMessage.textContent = 'Disconnected. Click Start to restart.';
        micIcon.style.boxShadow = 'none';
        cleanUp();
      };

      webSocket.onerror = (err) => {
        console.error('WebSocket error', err);
        statusMessage.textContent = 'WebSocket Error. See console.';
        if (webSocket) webSocket.close();
      };
    }

    function handleWebSocketMessage(event) {
      try {
        const msg = JSON.parse(event.data);
        const liveOutput = msg.bidiGenerateContentRealtimeOutput;
        if (!liveOutput) return;

        // Live transcription updates
        if (liveOutput.liveTranscription) {
          const text = liveOutput.liveTranscription.text || '';
          const isFinal = !!liveOutput.liveTranscription.is_final;
          if (text) {
            currentTranscription = text;
            transcriptionOutput.textContent = isFinal ? text : text + '...';
          }
        }

        // Voice activity / utterance detection -> trigger generation
        if (liveOutput.voiceActivity) {
          const state = liveOutput.voiceActivity.utterance_state;
          if (state === 'END' && !isGenerating) {
            statusMessage.textContent = 'Interviewer paused. Generating suggestion...';
            suggestionsContainer.classList.remove('hidden');
            loadingIndicator.classList.remove('hidden');
            suggestionsList.innerHTML = '';
            isGenerating = true;
          }
          if (state === 'START' && isGenerating) {
            suggestionsContainer.classList.add('hidden');
            loadingIndicator.classList.add('hidden');
            suggestionsList.innerHTML = '';
            isGenerating = false;
          }
        }

        // Live generation stream for suggestions
        if (liveOutput.liveGeneration && isGenerating) {
          const textChunk = liveOutput.liveGeneration.text_update || '';
          const done = !!liveOutput.liveGeneration.is_done;

          if (textChunk) {
            loadingIndicator.classList.add('hidden');

            // naive split into bullets
            const existing = suggestionsList.innerHTML || '';
            const combined = (existing ? existing.replace(/<\/?li>/g,'') : '') + textChunk;
            const points = combined.split(/[\n\-\*\•]+/).map(p => p.trim()).filter(Boolean);
            suggestionsList.innerHTML = points.map(p => `<li>${p}</li>`).join('');
          }

          if (done) {
            isGenerating = false;
            statusMessage.textContent = 'Listening for Interview Audio...';
          }
        }
      } catch (err) {
        console.error('handleWebSocketMessage error', err);
      }
    }

    // === Audio processing (Using AudioWorklet) ===

    async function startAudioProcessing() {
      try {
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        
        // 1. Load the Worklet file
        await audioContext.audioWorklet.addModule('pcm-processor.js');

        // 2. Create the Worklet Node
        workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');

        const mediaSource = audioContext.createMediaStreamSource(mediaStream);
        
        // 3. Handle messages from the Worklet thread (containing the PCM data)
        workletNode.port.onmessage = (event) => {
            if (!webSocket || webSocket.readyState !== WebSocket.OPEN) return;
            
            const pcm16DataBuffer = event.data;
            if (!pcm16DataBuffer || pcm16DataBuffer.byteLength === 0) return;

            // IMPORTANT: embed base64-encoded audio in JSON
            const base64Audio = bufferToBase64(pcm16DataBuffer); 

            const audioPayload = {
                bidiGenerateContentRealtimeInput: {
                    audio: {
                        audioBytes: base64Audio
                    }
                }
            };

            try {
                webSocket.send(JSON.stringify(audioPayload));
            } catch (err) {
                console.error('Failed to send audio payload', err);
            }
        };

        // 4. Connect the stream to the Worklet
        mediaSource.connect(workletNode);
        // Connect to destination (optional, but often used to keep the node alive)
        workletNode.connect(audioContext.destination); 
        
      } catch (err) {
        console.error('startAudioProcessing error', err);
        statusMessage.textContent = 'Microphone error. Grant permission or check device.';
        throw err;
      }
    }


    function cleanUp() {
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      
      if (workletNode) { 
        try { workletNode.disconnect(); } catch(e) {}
        workletNode = null;
      }
      
      if (audioContext) {
        audioContext.close().catch(() => {});
        audioContext = null;
      }
      if (webSocket) {
        try { webSocket.close(); } catch(e) {}
        webSocket = null;
      }

      isStreaming = false;
      isGenerating = false;
      toggleMicBtn.textContent = 'Start';
      suggestionsContainer.classList.add('hidden');
      suggestionsList.innerHTML = '';
      transcriptionOutput.textContent = '';
      currentTranscription = '';
      initialMessage.classList.remove('hidden');
      micIcon.style.boxShadow = 'none';
    }

    // === UI / Controls ===

    async function toggleSession() {
      if (isStreaming) {
        statusMessage.textContent = 'Stopping session...';
        cleanUp();
        statusMessage.textContent = 'Session stopped.';
        return;
      }

      try {
        // Fetch all necessary data from the backend
        const data = await fetchEphemeralToken(); 

        // Start audio capture
        await startAudioProcessing();

        // Then open websocket and pass the data object
        setupWebSocket(data); 

        toggleMicBtn.textContent = 'Stop';
      } catch (err) {
        console.error('toggleSession error', err);
        cleanUp();
      }
    }

    toggleMicBtn.addEventListener('click', toggleSession);

    window.addEventListener('beforeunload', () => cleanUp());

    window.onload = () => {
      statusMessage.textContent = 'Ready. Click Start.';
      suggestionsContainer.classList.add('hidden');
    };
  </script>
</body>
</html>
